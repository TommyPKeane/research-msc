%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Tommy P. Keane
% Master of Science Thesis
% Department of Electrical and Microelectronic Engineering
% Rochester Institute of Technology
%
% April 2011
%
%
%
% Funded By: Lenel Systems Inc., A UTC Fire & Security Corporation
%
% Algorithm Intellectual Property Owned By: Lenel Systems Inc.
%
%
% http://www.tommypkeane.com
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 3
%
% SECTION 1.2: Affine Transform Search
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT

The affine transformation is defined by 4 distinct operations: scale, skew, rotation, and translation. Skew, translation, and scale can each be defined separately for the rows and for the columns of the image. This allows for more complex deformations but from the understanding of projective geometry, it would be more accurate to model realistic deformations by different amounts of scaling or skew variation across the view, not just a constant transformation equally along all the rows versus another equally across all the columns. This approach was not investigated because the current implementation of the algorithm already performs an exhaustive translation search and research into more complex transformations proved to unacceptably increase computation costs.

The most drastic assumption made by the WFMI algorithm is in the utilization of an affine-only search for image registration in views with transformations significantly outside affine constraints. This was an assumption made mostly for computational efficiency and simplicity, it is not an assumption based on a strong mathematical or conceptual understanding such as the assumptions discussed in Chapter 2. That being said, it is an efficient and robust means of generating estimates for accurate registration between overlapping views of complex scenes. In the actual implementation the affine transform was not usually implemented but a similarity transform (rotation, scale, and translation only) was used instead. It was found that in most scenarios, skew had little to no effect on providing a more convincing view and so that dimension of the search space was ignored. That is in practice only; in the design of the algorithm it is part of the full implementation. It was also found that between rotation and scaling, rotation proved the more important factor in creating a convincing view. The reasons behind this will be discussed in Chapter 4, as the rest of the algorithm must be explored first.

The affine (or similarity) transform search has been implemented in practice as a limited search in rotation, scale, and skew, with an exhaustive translation search. While it is maintained that this is a fully automatic algorithm, there is the practical consideration that between two overlapping views of a realistic scene there will not be views rotated by 90\textsuperscript{o} between them or scaled by 100\%. With that understanding, then, it was found through testing that realistic scenes with average overlap (usually 20-40\%) were in the range of $-15^{o}$ to $15^{o}$ of rotation and a scale factor of $0.8$ to $1.2$ was a generous range. While skew was usually ignored in the actual implementation, it can be an overcompensating factor and so while a range similar to the scale factor seems reasonable, it was usually restricted to about half the range of the scale factor, to maintain more reasonable results. The main problem with skew is that when deforming an entire image, it did not prove to be a reliable estimate for the effects of parallax and projective distortion in views of realistic scenes. Again, more of the algorithm needs to be discussed first so this will be elaborated on in the following chapter, but skew tended to promote mis-registration of low entropy regions as it allows the view to distort in an unrealistic manner.

The search itself is done as two stages. First one image is chosen as the reference image, the affine homography to be found will be a transformation for the second image onto this first image. The homography should be invertible and so the reference image could be transformed onto the second image using the inverse of the transform matrix, but this is more of a mathematical consideration than a practical one. In the case of a full affine search, the skew, scale, and rotation ranges are defined (as was mentioned, it is a practical limitation) and the search space is the three-dimensional space covered by these ranges. At this point translation is being ignored. Once the reference image (see left path of Figure \ref{flowchart} and the affine (minus translation) search space is defined, the second image (following the right path of Figure \ref{flowchart}) undergoes a transformation in this space. For this implementation there was no enhancement made to the navigation of the search space and so the skew-scale-rotation-space is searched linearly, which may not be the most accurate method. Once the second image has undergone the transformation it is now a temporary new image and it is passed to the correspondence stage (Section 3.1.3) with the original reference image. Note, this is done at the top level of the Gaussian pyramid created for the hierarchical search, and this is done on the 64-bit color gradient image maps.

The flowchart of Figure \ref{flowchart} is conceptually accurate but in practice, especially during the transition from MATLAB\textsuperscript{\textregistered} to OpenCV, it was found that the Gaussian pyramid reduction adds superfluous quantization to the ``edge'' images. This was found in the OpenCV implementation because C++ is a hard-typed language and MATLAB\textsuperscript{\textregistered} defaults to 64-bit data, so in the prototyping stage it was not initially recognized that the $b_{e}$-bit ``edge'' images were actually being stored as 64-bit images. What this requires then is to first build the Gaussian pyramid with the 64-bit gradient maps, assign the reference and search image maps, transform the search gradient map, and then once it is transformed it can quantized into the $b_{e}$-bit ``edge'' map.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END OF DOCUMENT

