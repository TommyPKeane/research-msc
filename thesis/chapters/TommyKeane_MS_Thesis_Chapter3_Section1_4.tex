%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Tommy P. Keane
% Master of Science Thesis
% Department of Electrical and Microelectronic Engineering
% Rochester Institute of Technology
%
% April 2011
%
%
%
% Funded By: Lenel Systems Inc., A UTC Fire & Security Corporation
%
% Algorithm Intellectual Property Owned By: Lenel Systems Inc.
%
%
% http://www.tommypkeane.com
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 3
%
% SECTION 1.4: Stitching and Blending
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT

At this point in the algorithm, following along in Figure \ref{algorithmFlowchart}, the optimal affine homography has been found for the search image (the right-hand path) to allow it to be transformed into the image space of the reference image (the left-hand path). Having this optimal affine homography allows the original color image, for the search image, to be transformed, and the color image for the reference image can be taken. Now with these two color images the problem to face is how to blend these views now that they are overlapping. Conceptually (ignoring the 3 color channels) the images can be seen as two layers overlapping each other in the image space of the reference image, no longer in disparate image spaces. Thinking of these images as layers, there will be one image in front of the other, covering a portion of the second image, and that portion being covered is the overlap region. The simplest method would be to avoid the blending, cut the images at some halfway point in the overlap region, and place them side by side, essentially ``flatten'' the layers. The problem this presents is that if the homography that was found is not perfect, the stitch line (this straight line cut) will be clearly visible as objects in the views of the scene will not be spatially coherent across the seam. Common errors will be duplicated objects (``ghosting'') or jagged breaks in the objects. If a better homography could be found, perhaps through an iterative process or through the use of reapplying the algorithm to objects in motion in the scene, then the ``ghosting'' or ``jumps'' could be minimized. They should be non-existent only if the homography is perfectly accurate for the views. However, there is another possibility for error: illumination variation.

Using views at unknown locations in realistic scenarios provides a huge range of variability in the artifacts, noise, and illumination source(s) for the views. For example, in an outdoor scene with cameras placed on a low rooftop, there could be a camera placed by/under a tree while the other camera is out in the open. When there is inclement weather or at certain times of the day (changing position of the sun), the views will have very different illuminations and artifacts. The view out in the open will be extremely susceptible to rain, snow, and saturation from sunlight and the reflection of the sun off of objects such as cars, windows, or metal structures (all ubiquitous in real surveillance scenarios). The view under the tree will be shaded and significantly darker, it will be protected from weather effects, but could be blocked by leaves or branches in windy conditions. There are numerous other ways in which two cameras in a realistic indoor or outdoor scenario could be susceptible to extremely varied artifacts. So, even if an accurate homography (or accurate estimate) is produced, there needs to be a method to automatically overcome these artifacts, otherwise the view will be very unconvincing. But given that these realistic scenes are well-beyond the affine constraints and the transformation applied is only an affine transformation, there cannot be a perfectly accurate homography for these scenes. That means that before the algorithm even runs, it is known that at the outcome of the registration, there is no guarantee of pixel correspondence. Without pixel correspondence in the overlap region, there is no way
to determine the illumination disparity between the views accurately. Think of overlapping views of a scene with a red pickup truck in it. Once these views are registered and the search image is transformed into the reference image space (think again of the images as layers), the overlap region would ideally contain the same view of the same red truck. Using the concept of layering the images in reference image space, the pixels on the truck should be in line in this third dimension (the layer dimension), and so with any weather or illumination artifacts/disparities between the views, the corresponding pixels on the truck are desired to be at the same color value. So, very easily a difference could be found between these pixels and a color mean could be determined and the two views could be adjusted to this mean before they are blended, so that they appear to have the same illumination and weather artifacts that affected the illumination will have been overcome. But the WFMI algorithm produces an estimate of the overlap region, so thinking of these images in the layer-space means that the red truck won't line up. Trying to take color differences would require knowing which pixels to use, but having that knowledge only comes from an accurate registration, which is not available. Very quickly it becomes clear that any color mean approach to overcome the illumination variation is an ill-posed problem when dealing with registration estimates.

While the WFMI algorithm is not providing a perfect registration (for the general realistic scenario), it is providing an accurate estimate. Therefore if there is a red truck in both views, then that red truck will be in the overlap region of both the reference image and the transformed search image, but it will not be at the exact same location in both of those views. So the illumination variation information is there, but there isn't a simple pixel difference method available to extract it. However, the multi-resolution spline blending algorithm from \cite{Burt1983} is perfectly suited for this scenario. While based on a more rigorous Fourier analysis than is applicable to this discussion, conceptually this method can be thought of as a combining method for the frequency content in the images, not just in the overlap regions. Therefore, even with an extremely large illumination variation between the views, they can be blended accurately as the combination of their frequency content can be thought of as a more complex and complete type of method similar to finding a mean illumination between the views. However, as will be clarified in the discussion of the implementation, this does not just find a mean and modify both views to this fixed singular mean (or difference from the mean), it will blend the images by the strongest amount at the seam and by the smallest amount at the boundaries furthest from the seam. Again, this is ideal for this situation because the goal is to create a convincing view. The overlap region (where the seam will be placed) is the only portion of the views that should correspond, essentially it is unknown and unimportant as to what exists outside the overlap regions in the views. So with a blending algorithm that has its greatest effect at the seam, \ie{ }in the overlap region, and its strength decays away from the seam, the images will remain close to their original state but will slowly and visually acceptably blend (in illumination) around the overlapping region. Now, even if there are registration errors (``ghosting'' and ''jumps''), the views' illuminations will blend very well and will present a stronger cognitive link to the correspondence between these views even as only an estimate. Again, think of the red pickup truck. If a given scenario results in the seam going through the pickup truck and there is a ``jump'' across the scene, it will look very unconvincing and slightly confusing if one portion of the truck is bright red while the other portion is a very dark red. There could even be a very likely scenario where the truck seems to be two different colors, and with spatial discontinuity it may not even seem (at first glance) to be the same truck. But if the illumination is constant in this region, the color of the truck would be the same and there would just be a spatial discontinuity, thus making the perceptual task of the observer much easier in believing that this is an accurate registration estimate of the views. And again, these views could be enhanced by the temporal information (through iteration or the use of objects in motion) and so spatial discontinuities can be overcome but without consistent illumination the view will not be convincing, especially if objects are in motion across a seam with spatial discontinuities. This will not only be applicable to human observers but automated image processing techniques as well, such as segmentation or tracking. Remember that a tracking or segmentation algorithm will be viewing the panorama as a single view, and if there is extreme illumination variation across the seam then it wouldn't be unreasonable to assume that, for a singular view, color variations correspond to different objects or regions. Many problems can arise from color variation much more often than slight spatial inconsistencies.

To implement the method from \cite{Burt1983}, the easiest method is to transform the search image into the reference image space, and then to pad the images with zero values (ignored/null values) so that if they were thought of as layers, then they would be the same size and the translation between the views would be taken care of by the zero-padding. Imagine laying one image on top of the other (the transformed search image on top of the reference image), translate the search image, now draw a boundary around the entire 2D rectangular space that encapsulates both images, and then separate the images as layers and fill the boundary (for each image) with zero values. Now they will be the same size, the overlap regions will line up (if layered), and the rest of the images will be zero. This makes computation simple because the images are now the same size and the location of the seam will be complimentary with respect to the columns and/or rows in each image. However, this does add in a significant amount of new data, even though all that data is zero. But again, the multi-resolution spline blending algorithm can be thought of as a decaying illumination mean adjustment, and now both images have had a potentially drastic mean shift because of all the zeros added. What is observed in the results is that the images are slightly darker, and there is superfluous blending from each image into the zero padded regions of the other image. These are implementation artifacts that do not cause significant detriment to the results and are significantly complex to solve given the goals and constraints of the algorithm unless data from the views are thrown away. This would be unwise as this is a two image algorithm, but is applicable to many images. The registration process for multiple images would be to register two images, then continue through the other images adding them on to the image(s) that are already registered. If data is thrown out in any initial registration, subsequent registrations may not be available because they might have corresponded to those lost regions. So these artifacts are entirely acceptable until a more intelligent improvement in the stitching seam choice can be appended to the algorithm.

The multi-resolution spline is based on a Laplacian pyramid reduction (similar to the Gaussian pyramid reduction), where (after the two images' pyramids are generated) once the stitching seam is defined, the images are cut and combined along that seam, and then the reverse of the Laplacian pyramid reduction operation is performed to the joined images. This Laplacian pyramid combination is thought of as combining the frequency content of the images and then rebuilding the panoramic image with the frequency content of the images together. The pyramid generation is the most computationally complex portion of the blending, but given that it based on the same means of calculating the Gaussian pyramid reduction, it could ostensibly be made quite efficient by reuse of those same calculations. It is by no means an impassable limitation, but it has not been implemented in its most efficient method in the current algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END OF DOCUMENT

