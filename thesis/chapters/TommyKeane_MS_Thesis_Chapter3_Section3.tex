%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Tommy P. Keane
% Master of Science Thesis
% Department of Electrical and Microelectronic Engineering
% Rochester Institute of Technology
%
% April 2011
%
%
%
% Funded By: Lenel Systems Inc., A UTC Fire & Security Corporation
%
% Algorithm Intellectual Property Owned By: Lenel Systems Inc.
%
%
% http://www.tommypkeane.com
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 3
%
% SECTION 3: Affine Transform Search
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT

The affine transformation is defined by 4 distinct operations: scale, skew, rotation, and translation. Skew, translation, and scale can each be defined separately for the rows and for the columns of the image. This allows for more complex deformations but from the understanding of projective geometry, it would be more accurate to model realistic deformations by different amounts of scaling or skew variation across the view, not just a constant transformation equally along all the rows versus another equally across all the columns. This approach was not investigated because the current implementation of the algorithm already performs an exhaustive translation search and research into more complex transformations proved to unacceptably increase computation costs.

The most drastic assumption made by the WFMI algorithm is in the utilization of an affine-only search for image registration in views with relationships significantly outside affine constraints. This was an assumption made mostly for computational efficiency and simplicity, it is not an assumption based on a strong mathematical or conceptual understanding such as the assumptions discussed in Chapter 2. That being said, it is an efficient and robust means of generating estimates for accurate registration between overlapping views of complex scenes. In the actual implementation the affine transform was not usually implemented but a similarity transform (rotation, scale, and translation only) was used instead. It was found that in most scenarios, skew had little to no effect on providing a more convincing view and so that dimension of the search space was ignored. That is in practice only; in the design of the algorithm it is part of the full implementation. It was also found that between rotation and scaling, rotation proved the more important factor in creating a convincing view. The reasons behind this will be discussed with the results presented in Chapter 4, as the rest of the algorithm must be explored first.

The affine (or similarity) transform search has been implemented in practice as a limited search in rotation, scale, and skew, with an exhaustive translation search. While it is maintained that this is a fully automatic algorithm, there is the practical consideration that between two overlapping views of a realistic scene there will not be views rotated by 90\textsuperscript{o} between them or scaled by 100\%. With that understanding, then, it was found through testing that realistic scenes with average overlap (usually 20-40\%) were in the range of $-15^{o}$ to $15^{o}$ of rotation and a scale factor of $0.8$ to $1.2$ was a generous range. While skew was usually ignored in the actual implementation, it can be an overcompensating factor and so while a range similar to the scale factor seems reasonable, it was usually restricted to about half that range, to maintain more reasonable results. The main problem with skew is that when deforming an entire image, it did not prove to be a reliable estimate for the effects of parallax and projective distortion in views of realistic scenes. Again, more of the algorithm needs to be discussed first, so this will be elaborated on in the following chapter, but skew tended to promote mis-registration of low entropy regions as it allows the view to distort in an unrealistic manner.

The search itself is done as two stages. First, one image is chosen as the reference image; the affine homography to be found will be a transformation for the second image (the search image) onto this first image (the reference image). The homography should be invertible and so the reference image could be transformed onto the search image using the inverse of the transform matrix, but this is more of a mathematical consideration than a practical one \cite{Hartley2003}. In the case of a full affine search, the skew, scale, and rotation ranges are pre-defined (as was mentioned, this is a practical limitation) and the search space is the three-dimensional space covered by these ranges. At this point translation is being ignored. Once the reference image (following the left path of Figure \ref{algorithmFlowchart}) is chosen and the affine (without translation) search space is defined, the second image (following the right path of Figure \ref{algorithmFlowchart}) undergoes a transformation in this space. For this implementation there was no enhancement made to the navigation of the search space and so the skew-scale-rotation-space (affine search space) is searched linearly, which may not be the most accurate method. Once the second image has undergone the transformation it is now a temporary new image and it is passed to the correspondence stage (Section 3.4) along with the unmodified reference image for comparison. Note, this is done at the top level of the Gaussian pyramid created for the hierarchical search, and this is done on the 64-bit color gradient image maps.

The flowchart of Figure \ref{algorithmFlowchart} is conceptually accurate but in practice, especially during the transition from MATLAB\textsuperscript{\textregistered} to OpenCV, it was found that the Gaussian pyramid reduction adds superfluous quantization to the ``edge'' images. This was found in the OpenCV implementation because C++ is a hard-typed language and MATLAB\textsuperscript{\textregistered} defaults to 64-bit data; so in the prototyping stage it was not initially recognized that the $b_{e}$-bit ``edge'' images were actually being stored as 64-bit images. What this requires then is to first build the Gaussian pyramid with the 64-bit gradient maps, choose the reference and search images, transform the search gradient map, and once it is transformed it can quantized into the $b_{e}$-bit ``edge'' map. After choosing the reference image, it can be quantized as well. The correspondence stage (Section 3.4) is thus working on the reference image's ``edge'' map and the transformed search image's ``edge'' map, which are from the top of the Gaussian pyramid for the hierarchical search. In both implementations these images are padded with zero values (since zero is ignored as data) to be the same size.

The next section will discuss the implementation of translation, but a final remark is to note that in the overview of the algorithm it is important to keep in mind that the correspondence is occurring in the skew-scale-rotation-space. So for every combination of skew, scale, and rotation (every point in that discrete 3-D space) a peak from the filtered and weighted mutual information map will be found and will be compared to the rest of the space. In implementation, to save memory, each time the temporary image is created by the transformation of the search image, it is replaced when moving to the next transformation in the affine search space. In Figure \ref{algorithmFlowchart} there is a second transformation after the WFMI stage (the correspondence search), and this second transformation is defining the final transformation for the search image based on the results from the correspondence search. Again, since each transformation was thrown out during the search, once the optimal peak is found from the affine search, the corresponding affine transform must be reapplied to the search image, while the reference image still remains unmodified.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END OF DOCUMENT

