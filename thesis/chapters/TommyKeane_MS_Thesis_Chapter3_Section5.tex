%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Tommy P. Keane
% Master of Science Thesis
% Department of Electrical and Microelectronic Engineering
% Rochester Institute of Technology
%
% April 2011
%
%
%
% Funded By: Lenel Systems Inc., A UTC Fire & Security Corporation
%
% Algorithm Intellectual Property Owned By: Lenel Systems Inc.
%
%
% http://www.tommypkeane.com
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 3
%
% SECTION 5: Stitching and Blending
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT

At this point in the algorithm, following along in Figure \ref{algorithmFlowchart}, the optimal affine homography has been found for the search image to allow it to be transformed into the image space of the reference image. Now with these two color images, transformed appropriately, the problem being faced is how to blend these views now that they are overlapping. Conceptually (ignoring the 3 color channels) the images can be seen as two layers overlapping each other in the image space of the reference image, no longer in disparate image spaces. Thinking of these images as layers, there will be one image in front of the other, covering a portion of the second image, and that portion being covered is the overlap region, as depicted in Figure \ref{layerImages}.

\begin{figure}
\centering
\includegraphics[width=.9\columnwidth]{OverlapFloatRooftop_001}
\caption{Depiction of Images as Layers in single Image Coordinate Space}
\label{layerImages}
\end{figure}

The simplest method would be to avoid the blending, cut the images at some halfway point in the overlap region, and place them side by side, essentially ``flatten'' the layers. The problem this presents is that if the homography that was found is not perfect, the stitch line will be clearly visible as objects in the views of the scene will not be spatially coherent across the seam. Common errors will be duplicated objects (``ghosting'') or jagged breaks (``jumps'') in the combined view of the scene. If a more accurate homography could be found, perhaps through an iterative process or through the use of reapplying the algorithm to objects in motion in the scene, then the ``ghosting'' or ``jumps'' could be minimized. Yet, these errors will only be non-existent if the homography is perfectly accurate for the views, overcoming all parallax and occlusion disparity concerns. And then there is another possible source of error, beyond the spatial concerns, in the potential illumination variation and environmental artifacts (weather, \textit{et cetera}).

Using views at unknown locations in realistic scenarios provides a huge range of variability in the artifacts, noise, and illumination source(s) for the views. For example, in an outdoor scene with cameras placed on a low rooftop, there could be a camera placed by/under a tree while the other camera is out in the open. When there is inclement weather or at certain times of the day (changing position of the sun), the views will have very different illuminations and artifacts. So, even if an accurate homography (or accurate estimate) is produced, there needs to be a method to automatically overcome these artifacts, otherwise the view will be very unconvincing. But given that these realistic scenes are well-beyond the affine constraints, and the transformation applied is only an affine transformation, there is nothing to support any assumption of a perfectly accurate homography in a realistic scenario. That means that before the algorithm even runs, it is known that at the outcome of the registration, there is no guarantee of pixel correspondence. Without pixel correspondence in the overlap region, it becomes an ill-posed problem to determine the illumination disparity between the views accurately.

An example to illustrate the scenario is to think of overlapping views of a scene with a red pickup truck in it. Once these views are registered and the search image is transformed into the reference image space, the overlap region would ideally contain the same view of the same red truck. Using the concept of layering the images in reference image space, the pixels on the truck should be in line in this third dimension (the layer dimension), and so with any weather or illumination artifacts/disparities between the views, the corresponding pixels on the truck are desired to be at the same color value. So, very easily a difference could be found between these pixels and a color mean could be determined and the two views could be adjusted to this mean before they are blended, so that they appear to have the same illumination and weather artifacts that affected the illumination will have been overcome. But the WFMI algorithm produces an estimate of the overlap region because of the complications presented by the positions of the views and the structure of the scene. So, in thinking of these images in the layer-space, that means that the red truck won't line up pixel-to-pixel, but the disparate views of the red truck will now be in the same region. Trying to take color-mean differences would require knowing which pixels to use, but that is exactly what is not available. Understanding the structure and nature of the algorithm and the views pushed the development towards the implementation of the multi-resolution spline blending algorithm, which works based on a frequency content blending of any two views. The scene content, the red truck, is aligned through an estimate, so its frequency content should be relatively similar, or it is desired to be so. Thus by blending in the frequency domain, the misaligned spatial structure becomes a secondary concern, and is no longer a hindrance to creating a convincing view in terms of illumination and contrast variations.

When implementing the multi-resolution spline blending algorithm from \cite{Burt1983} in the WFMI algorithm, first the the search image is transformed into the reference image space. To aid implementation the images are padded with zero values (the ignored/null values) so that if they were thought of as layers, then they would be the same size and the translation between the views is implemented directly by the zero-padding. See Figure \ref{OverlappingViews} for a visual representation of the padding and a transparent overlapping view.

\begin{figure}[h]
\centering
\includegraphics[width=.9\columnwidth]{OptimalAffine_002}
\caption{Rooftop Scene (a) Left View Padded, (b) Right View Padded, (c) Overlapping Padded and Transformed Views}
\label{OverlappingViews}
\end{figure}

This makes computation simple because the images are now the same size and the location of the seam will be complimentary with respect to the columns and/or rows in each image. However, this does add in a significant amount of new data, even though all that data is zero. But again, the multi-resolution spline blending algorithm can be thought of as a decaying illumination mean adjustment, and now both images have had a potentially drastic mean shift because of all the zero values added to the image content. What is observed in the results is that the images are slightly darker, and there is superfluous blending from each image into the zero padded regions of the other image. These are implementation artifacts that do not cause significant detriment to the results and over-complicating the algorithm to avoid them was not a priority in this initial development. The registration process for multiple images would be to register two images, then continue through the other images adding them on to the image(s) that are already registered. Though consideration must be made if data is thrown out in any initial registration, as subsequent registrations may not be available because they might have corresponded to those lost regions.

The multi-resolution spline algorithm is based on a Laplacian pyramid reduction, generated by building a Gaussian pyramid, and then taking the difference between pairs of layers. The bottom layer of a Laplacian pyramid is the difference between the bottom two levels of the Gaussian pyramid. Since the Gaussian pyramid layers are subsampled, the smaller of the two layers will need to be upsampled in order to perform a pixel-to-pixel difference between the layers. Once the Laplacian pyramid is defined for each image, a stitching seam is defined and the images are cut and combined along that seam, as shown in Figure \ref{laplacianBlend}.

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{MultiResolutionSpline_002}
\caption{Laplacian Pyramid Blending Diagram}
\label{laplacianBlend}
\end{figure}

The choice of the stitching seam can be made through an analysis of the image, but to better show the accuracy of the results, this implementation choice to find the seam as a vertical line in the center of the overlap region. Joining each layer of the two Laplacian pyramids creates the panorama's Laplacian pyramid. In following the development of the Laplacian from the Gaussian, the RGB panorama can be found by upsampling and then summing each layer of the Laplacian pyramid in pairs, starting with the top layer. The exact reverse of the Gaussian to Laplacian operation. This Laplacian pyramid combination is thought of as combining the frequency content of the images and then rebuilding the panoramic image with the frequency content of the images together. The pyramid generation is the most computationally complex portion of the blending, but given that it based on the same means of calculating the Gaussian pyramid reduction, it could ostensibly be made quite efficient by reuse of those same calculations. It is by no means an impassable limitation, but it has not been implemented in its most efficient method in the current algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END OF DOCUMENT

